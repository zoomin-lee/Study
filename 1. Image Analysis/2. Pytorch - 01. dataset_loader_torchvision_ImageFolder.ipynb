{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "no_cuda = False\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "### datasets.ImageFolder\n",
    "label별로 폴더 안에 데이터가 저장되어 있는 경우 사용\n",
    "- 단, grayscale 데이터는 사용 불가 : https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py#L157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../dataset/mnist_png/training'\n",
    "test_path = '../dataset/mnist_png/testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    root=train_path,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(\n",
    "    root=test_path,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jumin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313317\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.242190\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.119130\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.863133\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.300581\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.798503\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.741911\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.553898\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.339050\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.345268\n",
      "\n",
      "Test set: Average loss: 0.3760, Accuracy: 8915/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.340321\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.203217\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.448332\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.404151\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.385495\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.313359\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.194417\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.240319\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.397526\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.114830\n",
      "\n",
      "Test set: Average loss: 0.2348, Accuracy: 9324/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.387451\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.342533\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.283022\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.278762\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.135737\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.356536\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.143100\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.144576\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.182755\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.251110\n",
      "\n",
      "Test set: Average loss: 0.1774, Accuracy: 9508/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.130154\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.184836\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.175113\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.221711\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.189280\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.123661\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.166235\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.108849\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.140939\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.130751\n",
      "\n",
      "Test set: Average loss: 0.1425, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.162393\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.175920\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.138097\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.132889\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.161141\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.112812\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.077583\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.235145\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.287998\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.221453\n",
      "\n",
      "Test set: Average loss: 0.1253, Accuracy: 9620/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # backpropagation 계산하기 전에 0으로 기울기 계산\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    # Test mode\n",
    "    model.eval()  # batch norm이나 dropout 등을 train mode 변환\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # pred와 target과 같은지 확인\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
