{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"59. Seq2Seq 신경망 구현 및 학습_실습.ipynb","provenance":[{"file_id":"15wQ-wscKkeuBndKpJcTN-HpRIrOrTcDH","timestamp":1630066636507}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FPtVeG-rjIJr","executionInfo":{"status":"ok","timestamp":1630067262429,"user_tz":-540,"elapsed":6911,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}},"outputId":"26bceeb1-8188-4164-ef05-375d15c00995"},"source":["! pip install konlpy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 40.3 MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 32.4 MB/s \n","\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcPv7EcwjLDa","executionInfo":{"status":"ok","timestamp":1630067281616,"user_tz":-540,"elapsed":19195,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}},"outputId":"c418ae62-cac6-4c5b-f21c-d17fc7ee8fdc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W987uOEtiYJ_"},"source":["## Seq2seq 신경망 구현 및 학습"]},{"cell_type":"code","metadata":{"id":"jDNL8kqiiYKN","executionInfo":{"status":"ok","timestamp":1630067283830,"user_tz":-540,"elapsed":2223,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["import random\n","import tensorflow as tf\n","from konlpy.tag import Okt"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1BxkNfbiYKU"},"source":["## 하이퍼 파라미터"]},{"cell_type":"code","metadata":{"id":"hR_pz9kiiYKY","executionInfo":{"status":"ok","timestamp":1630067283834,"user_tz":-540,"elapsed":8,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["EPOCHS = 200\n","NUM_WORDS = 5000"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQUczD2SiYKa"},"source":["## Encoder"]},{"cell_type":"code","metadata":{"id":"ZkYtVzd1iYKe","executionInfo":{"status":"ok","timestamp":1630067284229,"user_tz":-540,"elapsed":401,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["class Encoder(tf.keras.Model):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) \n","        # 5000개의 단어를 64크기의 vector로 embedding\n","        \n","        self.lstm = tf.keras.layers.LSTM(512, return_state=True)\n","        '''\n","          return_state : return하는 output에 최근의 state를 더해주냐에 대한 옵션 \n","                         즉, hidden state와 cell state를 출력해주기 위한 옵션임 \n","        '''\n","\n","    def call(self, x, training=False, mask=None):\n","        x = self.emb(x)\n","        _, h, c = self.lstm(x)\n","        return h, c"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtNrBHPHiYKh"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"7cvbO3WFiYKl","executionInfo":{"status":"ok","timestamp":1630067284229,"user_tz":-540,"elapsed":13,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["class Decoder(tf.keras.Model):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n","        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n","        '''\n","        return_sequences : return할 output을 full sequence 또는 마지막에만 출력할지 결정하는 옵션\n","        - true : full sequence 출력\n","        - false : 마지막에만 출력\n","        '''\n","        self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax')\n","\n","    def call(self, inputs, training=False, mask=None):\n","        x, h, c = inputs\n","        x = self.emb(x)\n","        x, h, c = self.lstm(x, initial_state=[h, c])\n","        # initial_state : 셀의 첫번쨰 호출로 전달될 초기 상태의 텐서 목록\n","        return self.dense(x), h, c"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bOClsgH2iYKp"},"source":["## Seq2seq"]},{"cell_type":"code","metadata":{"id":"gmJBMT3jiYKs","executionInfo":{"status":"ok","timestamp":1630067284230,"user_tz":-540,"elapsed":12,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["class Seq2seq(tf.keras.Model):\n","    def __init__(self, sos, eos):\n","        super(Seq2seq, self).__init__()\n","        self.enc = Encoder()\n","        self.dec = Decoder()\n","        self.sos = sos\n","        self.eos = eos\n","\n","    def call(self, inputs, training=False, mask=None):\n","        if training is True:\n","            x, y = inputs\n","            h, c = self.enc(x)\n","            y, _, _ = self.dec((y, h, c))\n","            # shifted output, hidden state, cell state을 초기값으로 입력 받음\n","            # y : 전체 문장\n","            return y\n","        else:\n","            x = inputs\n","            h, c = self.enc(x)\n","            y = tf.convert_to_tensor(self.sos)\n","            # decoder 단에 제일 먼저 sos를 넣어주게끔 tensor화 시킴\n","            y = tf.reshape(y, (1, 1))\n","\n","            seq = tf.TensorArray(tf.int32, 64)\n","\n","            for idx in tf.range(64):\n","                y, h, c = self.dec([y, h, c])\n","                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n","                '''\n","                예측한값을 다시 다음 step의 입력으로 넣어주기 위한 작업\n","                : 위의 출력으로 나온 y는 softmax를 지나서 나온 값이므로 가장 높은 값의 index값을 tf.int32로 형변환해주고\n","                TensorArray seq의 idx에 y를 추가해준다.\n","                '''\n","\n","                y = tf.reshape(y, (1, 1))\n","                # 실제로 네트워크를 사용할 때 Batch를 고려해서 사용해야 하기 때문에 (1,1)으로 설정해 준다.\n","\n","                seq = seq.write(idx, y)\n","\n","                if y == self.eos:\n","                    break\n","\n","            return tf.reshape(seq.stack(), (1, 64))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugFFrKxyiYKt"},"source":["## 학습, 테스트 루프 정의"]},{"cell_type":"code","metadata":{"id":"P-j-xdudiYKu","executionInfo":{"status":"ok","timestamp":1630067284231,"user_tz":-540,"elapsed":12,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["# Implement training loop\n","@tf.function\n","def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n","    output_labels = labels[:, 1:]\n","    shifted_labels = labels[:, :-1]\n","    with tf.GradientTape() as tape:\n","        predictions = model([inputs, shifted_labels], training=True)\n","        loss = loss_object(output_labels, predictions)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    train_loss(loss)\n","    train_accuracy(output_labels, predictions)\n","\n","# Implement algorithm test\n","@tf.function\n","def test_step(model, inputs):\n","    return model(inputs, training=False)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxcouTuZiYKv"},"source":["## 데이터셋 준비\n"]},{"cell_type":"code","metadata":{"id":"i9C8mp54iYKw","executionInfo":{"status":"ok","timestamp":1630067322753,"user_tz":-540,"elapsed":18217,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["dataset_file = '/content/drive/MyDrive/패스트캠퍼스/Part4) 딥러닝 3 STEP의 기초/dataset/chatbot_data.csv'  # acquired from 'http://www.aihub.or.kr' and modified\n","okt = Okt()\n","\n","with open(dataset_file, 'r') as file:\n","    lines = file.readlines()\n","    seq = [' '.join(okt.morphs(line)) for line in lines]\n","\n","questions = seq[::2]\n","answers = ['\\t ' + lines for lines in seq[1::2]]\n","\n","num_sample = len(questions)\n","\n","perm = list(range(num_sample))\n","random.seed(0)\n","random.shuffle(perm)\n","\n","train_q = list()\n","train_a = list()\n","test_q = list()\n","test_a = list()\n","\n","for idx, qna in enumerate(zip(questions, answers)):\n","    q, a = qna\n","    if perm[idx] > num_sample//5:\n","        train_q.append(q)\n","        train_a.append(a)\n","    else:\n","        test_q.append(q)\n","        test_a.append(a)\n","\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n","                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n","# filters의 default에는 \\t,\\n도 제거하기 때문에 이 둘을 제외하고 나머지 문장기호들만 제거하게끔 변경해주었다.\n","\n","tokenizer.fit_on_texts(train_q + train_a)\n","\n","train_q_seq = tokenizer.texts_to_sequences(train_q)\n","train_a_seq = tokenizer.texts_to_sequences(train_a)\n","\n","test_q_seq = tokenizer.texts_to_sequences(test_q)\n","test_a_seq = tokenizer.texts_to_sequences(test_a)\n","\n","x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n","                                                        value=0,\n","                                                        padding='pre',\n","                                                        maxlen=64)\n","y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n","                                                        value=0,\n","                                                        padding='post',\n","                                                        maxlen=65)\n","'''\n","y값에는 maxlen=65인 이유는 앞에 SOS와 뒤에 EOS가 붙어 있기 때문\n","학습시에는 앞에 하나를 떼므로 실제로는 64길이만 사용하는 것과 동일하게 된다.\n","'''\n","\n","x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n","                                                       value=0,\n","                                                       padding='pre',\n","                                                       maxlen=64)\n","y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n","                                                       value=0,\n","                                                       padding='post',\n","                                                       maxlen=65)\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39Z1Q4zZiYKy"},"source":["## 학습 환경 정의\n","### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"]},{"cell_type":"code","metadata":{"id":"xFLVK7vkiYK0","executionInfo":{"status":"ok","timestamp":1630067322754,"user_tz":-540,"elapsed":17,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}}},"source":["# Create model\n","model = Seq2seq(sos=tokenizer.word_index['\\t'],\n","                eos=tokenizer.word_index['\\n'])\n","\n","# Define loss and optimizer\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# Define performance metrics\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCUfRLB3iYK1"},"source":["## 학습 루프 동작"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"bo-Vr2vFiYK1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630067544981,"user_tz":-540,"elapsed":222241,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}},"outputId":"296b9eea-67ad-4365-ff15-92de2706d723"},"source":["for epoch in range(EPOCHS):\n","    for seqs, labels in train_ds:\n","        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n","\n","    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n","    print(template.format(epoch + 1,\n","                          train_loss.result(),\n","                          train_accuracy.result() * 100))\n","\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1, Loss: 4.106929302215576, Accuracy: 83.13361358642578\n","Epoch 2, Loss: 0.6438242793083191, Accuracy: 90.3900375366211\n","Epoch 3, Loss: 0.5867494344711304, Accuracy: 90.54667663574219\n","Epoch 4, Loss: 0.5632861256599426, Accuracy: 90.96961212158203\n","Epoch 5, Loss: 0.550795316696167, Accuracy: 91.16932678222656\n","Epoch 6, Loss: 0.5446475148200989, Accuracy: 91.13016510009766\n","Epoch 7, Loss: 0.5416964888572693, Accuracy: 91.10275268554688\n","Epoch 8, Loss: 0.5380862355232239, Accuracy: 91.09883880615234\n","Epoch 9, Loss: 0.5295473337173462, Accuracy: 91.14191436767578\n","Epoch 10, Loss: 0.5338311195373535, Accuracy: 91.09100341796875\n","Epoch 11, Loss: 0.5234279036521912, Accuracy: 91.10667419433594\n","Epoch 12, Loss: 0.5199558734893799, Accuracy: 91.14191436767578\n","Epoch 13, Loss: 0.5169908404350281, Accuracy: 91.21240234375\n","Epoch 14, Loss: 0.5063393712043762, Accuracy: 91.22806549072266\n","Epoch 15, Loss: 0.4985678493976593, Accuracy: 91.28681182861328\n","Epoch 16, Loss: 0.48776867985725403, Accuracy: 91.3690414428711\n","Epoch 17, Loss: 0.4670557677745819, Accuracy: 91.58051300048828\n","Epoch 18, Loss: 0.45542895793914795, Accuracy: 91.9525375366211\n","Epoch 19, Loss: 0.4480378329753876, Accuracy: 92.21882629394531\n","Epoch 20, Loss: 0.4375993311405182, Accuracy: 92.36372375488281\n","Epoch 21, Loss: 0.4274722635746002, Accuracy: 92.47337341308594\n","Epoch 22, Loss: 0.41944122314453125, Accuracy: 92.5399398803711\n","Epoch 23, Loss: 0.41566500067710876, Accuracy: 92.63392639160156\n","Epoch 24, Loss: 0.4151255786418915, Accuracy: 92.63001251220703\n","Epoch 25, Loss: 0.40331709384918213, Accuracy: 92.677001953125\n","Epoch 26, Loss: 0.4039732813835144, Accuracy: 92.7161636352539\n","Epoch 27, Loss: 0.3990582227706909, Accuracy: 92.72791290283203\n","Epoch 28, Loss: 0.39494138956069946, Accuracy: 92.79448699951172\n","Epoch 29, Loss: 0.39061811566352844, Accuracy: 92.81015014648438\n","Epoch 30, Loss: 0.38783571124076843, Accuracy: 92.7827377319336\n","Epoch 31, Loss: 0.3832392990589142, Accuracy: 92.86888885498047\n","Epoch 32, Loss: 0.38139981031417847, Accuracy: 92.88846588134766\n","Epoch 33, Loss: 0.3787877857685089, Accuracy: 92.95503997802734\n","Epoch 34, Loss: 0.37623849511146545, Accuracy: 92.98246002197266\n","Epoch 35, Loss: 0.3725978136062622, Accuracy: 93.02944946289062\n","Epoch 36, Loss: 0.37059900164604187, Accuracy: 93.06861114501953\n","Epoch 37, Loss: 0.36615195870399475, Accuracy: 93.12734985351562\n","Epoch 38, Loss: 0.36246809363365173, Accuracy: 93.19392395019531\n","Epoch 39, Loss: 0.3626628518104553, Accuracy: 93.17434692382812\n","Epoch 40, Loss: 0.35432448983192444, Accuracy: 93.2017593383789\n","Epoch 41, Loss: 0.35025957226753235, Accuracy: 93.21742248535156\n","Epoch 42, Loss: 0.3460504710674286, Accuracy: 93.26441192626953\n","Epoch 43, Loss: 0.34546926617622375, Accuracy: 93.31532287597656\n","Epoch 44, Loss: 0.3412669897079468, Accuracy: 93.33489990234375\n","Epoch 45, Loss: 0.3430585563182831, Accuracy: 93.34664916992188\n","Epoch 46, Loss: 0.3321111798286438, Accuracy: 93.40538787841797\n","Epoch 47, Loss: 0.33009764552116394, Accuracy: 93.41322326660156\n","Epoch 48, Loss: 0.32711130380630493, Accuracy: 93.46804809570312\n","Epoch 49, Loss: 0.3193037211894989, Accuracy: 93.49937438964844\n","Epoch 50, Loss: 0.32240375876426697, Accuracy: 93.4954605102539\n","Epoch 51, Loss: 0.317013144493103, Accuracy: 93.59727478027344\n","Epoch 52, Loss: 0.31240314245224, Accuracy: 93.62077331542969\n","Epoch 53, Loss: 0.30775484442710876, Accuracy: 93.64818572998047\n","Epoch 54, Loss: 0.3105904757976532, Accuracy: 93.72258758544922\n","Epoch 55, Loss: 0.30289432406425476, Accuracy: 93.78132629394531\n","Epoch 56, Loss: 0.2957715690135956, Accuracy: 93.88314819335938\n","Epoch 57, Loss: 0.29355910420417786, Accuracy: 93.87531280517578\n","Epoch 58, Loss: 0.2924928367137909, Accuracy: 93.92622375488281\n","Epoch 59, Loss: 0.28612786531448364, Accuracy: 94.00062561035156\n","Epoch 60, Loss: 0.28177136182785034, Accuracy: 94.0828628540039\n","Epoch 61, Loss: 0.2822309136390686, Accuracy: 94.0828628540039\n","Epoch 62, Loss: 0.27344805002212524, Accuracy: 94.19251251220703\n","Epoch 63, Loss: 0.2736469507217407, Accuracy: 94.25125122070312\n","Epoch 64, Loss: 0.27039170265197754, Accuracy: 94.2982406616211\n","Epoch 65, Loss: 0.26322808861732483, Accuracy: 94.38831329345703\n","Epoch 66, Loss: 0.26170605421066284, Accuracy: 94.45880126953125\n","Epoch 67, Loss: 0.25751087069511414, Accuracy: 94.50579071044922\n","Epoch 68, Loss: 0.25211891531944275, Accuracy: 94.63894653320312\n","Epoch 69, Loss: 0.25186946988105774, Accuracy: 94.6859359741211\n","Epoch 70, Loss: 0.2487691342830658, Accuracy: 94.67027282714844\n","Epoch 71, Loss: 0.24464526772499084, Accuracy: 94.85432434082031\n","Epoch 72, Loss: 0.240280881524086, Accuracy: 94.90914916992188\n","Epoch 73, Loss: 0.2354382872581482, Accuracy: 95.0109634399414\n","Epoch 74, Loss: 0.22849975526332855, Accuracy: 95.1088638305664\n","Epoch 75, Loss: 0.229421004652977, Accuracy: 95.14802551269531\n","Epoch 76, Loss: 0.22369447350502014, Accuracy: 95.3046646118164\n","Epoch 77, Loss: 0.21921707689762115, Accuracy: 95.35948944091797\n","Epoch 78, Loss: 0.21664117276668549, Accuracy: 95.44172668457031\n","Epoch 79, Loss: 0.21341706812381744, Accuracy: 95.51221466064453\n","Epoch 80, Loss: 0.209962859749794, Accuracy: 95.6140365600586\n","Epoch 81, Loss: 0.20658506453037262, Accuracy: 95.75501251220703\n","Epoch 82, Loss: 0.20215670764446259, Accuracy: 95.87640380859375\n","Epoch 83, Loss: 0.19969996809959412, Accuracy: 95.9586410522461\n","Epoch 84, Loss: 0.19258013367652893, Accuracy: 96.04087829589844\n","Epoch 85, Loss: 0.18905028700828552, Accuracy: 96.1505355834961\n","Epoch 86, Loss: 0.18798841536045074, Accuracy: 96.26801300048828\n","Epoch 87, Loss: 0.18276533484458923, Accuracy: 96.30717468261719\n","Epoch 88, Loss: 0.18185928463935852, Accuracy: 96.42857360839844\n","Epoch 89, Loss: 0.17476961016654968, Accuracy: 96.49514770507812\n","Epoch 90, Loss: 0.174617737531662, Accuracy: 96.51081085205078\n","Epoch 91, Loss: 0.17214509844779968, Accuracy: 96.62437438964844\n","Epoch 92, Loss: 0.16462630033493042, Accuracy: 96.72618865966797\n","Epoch 93, Loss: 0.16132423281669617, Accuracy: 96.7692642211914\n","Epoch 94, Loss: 0.1591433584690094, Accuracy: 96.82017517089844\n","Epoch 95, Loss: 0.15515154600143433, Accuracy: 96.875\n","Epoch 96, Loss: 0.15193022787570953, Accuracy: 96.97681427001953\n","Epoch 97, Loss: 0.15098081529140472, Accuracy: 97.06688690185547\n","Epoch 98, Loss: 0.14767250418663025, Accuracy: 97.16478729248047\n","Epoch 99, Loss: 0.1439390331506729, Accuracy: 97.1765365600586\n","Epoch 100, Loss: 0.1418028175830841, Accuracy: 97.21961212158203\n","Epoch 101, Loss: 0.139495387673378, Accuracy: 97.32926177978516\n","Epoch 102, Loss: 0.13725236058235168, Accuracy: 97.3723373413086\n","Epoch 103, Loss: 0.13367687165737152, Accuracy: 97.39582824707031\n","Epoch 104, Loss: 0.13166305422782898, Accuracy: 97.47415161132812\n","Epoch 105, Loss: 0.13005927205085754, Accuracy: 97.55247497558594\n","Epoch 106, Loss: 0.1265503466129303, Accuracy: 97.53681182861328\n","Epoch 107, Loss: 0.12418924272060394, Accuracy: 97.59555053710938\n","Epoch 108, Loss: 0.12083220481872559, Accuracy: 97.68953704833984\n","Epoch 109, Loss: 0.11879464983940125, Accuracy: 97.76786041259766\n","Epoch 110, Loss: 0.11514827609062195, Accuracy: 97.79918670654297\n","Epoch 111, Loss: 0.11401856690645218, Accuracy: 97.8422622680664\n","Epoch 112, Loss: 0.11126180738210678, Accuracy: 97.9088363647461\n","Epoch 113, Loss: 0.10911211371421814, Accuracy: 97.99498748779297\n","Epoch 114, Loss: 0.10705730319023132, Accuracy: 97.9401626586914\n","Epoch 115, Loss: 0.10493249446153641, Accuracy: 98.02631378173828\n","Epoch 116, Loss: 0.10206490010023117, Accuracy: 98.04981231689453\n","Epoch 117, Loss: 0.0998665988445282, Accuracy: 98.05372619628906\n","Epoch 118, Loss: 0.09934444725513458, Accuracy: 98.17121124267578\n","Epoch 119, Loss: 0.09635794162750244, Accuracy: 98.16337585449219\n","Epoch 120, Loss: 0.09475656598806381, Accuracy: 98.17903900146484\n","Epoch 121, Loss: 0.09243804961442947, Accuracy: 98.23778533935547\n","Epoch 122, Loss: 0.09099794179201126, Accuracy: 98.30435180664062\n","Epoch 123, Loss: 0.0891481339931488, Accuracy: 98.32002258300781\n","Epoch 124, Loss: 0.08691569417715073, Accuracy: 98.3474349975586\n","Epoch 125, Loss: 0.08455163985490799, Accuracy: 98.39442443847656\n","Epoch 126, Loss: 0.08295551687479019, Accuracy: 98.3983383178711\n","Epoch 127, Loss: 0.08163098245859146, Accuracy: 98.44141387939453\n","Epoch 128, Loss: 0.08063941448926926, Accuracy: 98.4453353881836\n","Epoch 129, Loss: 0.07919041812419891, Accuracy: 98.53931427001953\n","Epoch 130, Loss: 0.07673737406730652, Accuracy: 98.5275650024414\n","Epoch 131, Loss: 0.07539263367652893, Accuracy: 98.535400390625\n","Epoch 132, Loss: 0.07303191721439362, Accuracy: 98.60980224609375\n","Epoch 133, Loss: 0.07234770804643631, Accuracy: 98.62938690185547\n","Epoch 134, Loss: 0.06997736543416977, Accuracy: 98.66462707519531\n","Epoch 135, Loss: 0.0694718062877655, Accuracy: 98.71553802490234\n","Epoch 136, Loss: 0.06765580177307129, Accuracy: 98.67637634277344\n","Epoch 137, Loss: 0.06592405587434769, Accuracy: 98.76644897460938\n","Epoch 138, Loss: 0.06453976035118103, Accuracy: 98.76252746582031\n","Epoch 139, Loss: 0.06302868574857712, Accuracy: 98.80168914794922\n","Epoch 140, Loss: 0.061699025332927704, Accuracy: 98.83301544189453\n","Epoch 141, Loss: 0.06014029309153557, Accuracy: 98.82127380371094\n","Epoch 142, Loss: 0.058447644114494324, Accuracy: 98.88001251220703\n","Epoch 143, Loss: 0.056988511234521866, Accuracy: 98.87217712402344\n","Epoch 144, Loss: 0.05588774010539055, Accuracy: 98.90742492675781\n","Epoch 145, Loss: 0.05447414144873619, Accuracy: 98.93875122070312\n","Epoch 146, Loss: 0.052758973091840744, Accuracy: 98.9739990234375\n","Epoch 147, Loss: 0.0514870323240757, Accuracy: 98.9739990234375\n","Epoch 148, Loss: 0.05079139769077301, Accuracy: 99.06015014648438\n","Epoch 149, Loss: 0.049467504024505615, Accuracy: 99.08756256103516\n","Epoch 150, Loss: 0.048663921654224396, Accuracy: 99.0836410522461\n","Epoch 151, Loss: 0.0473661944270134, Accuracy: 99.06797790527344\n","Epoch 152, Loss: 0.04634210094809532, Accuracy: 99.1619644165039\n","Epoch 153, Loss: 0.044876374304294586, Accuracy: 99.14238739013672\n","Epoch 154, Loss: 0.04417340084910393, Accuracy: 99.15021514892578\n","Epoch 155, Loss: 0.04313766583800316, Accuracy: 99.21287536621094\n","Epoch 156, Loss: 0.041741788387298584, Accuracy: 99.21287536621094\n","Epoch 157, Loss: 0.04059571400284767, Accuracy: 99.24420928955078\n","Epoch 158, Loss: 0.039131756871938705, Accuracy: 99.25203704833984\n","Epoch 159, Loss: 0.038610585033893585, Accuracy: 99.2677001953125\n","Epoch 160, Loss: 0.03739443048834801, Accuracy: 99.29903411865234\n","Epoch 161, Loss: 0.036437369883060455, Accuracy: 99.32644653320312\n","Epoch 162, Loss: 0.03541665896773338, Accuracy: 99.32252502441406\n","Epoch 163, Loss: 0.03434909135103226, Accuracy: 99.34602355957031\n","Epoch 164, Loss: 0.03457753360271454, Accuracy: 99.37734985351562\n","Epoch 165, Loss: 0.0332244373857975, Accuracy: 99.41259765625\n","Epoch 166, Loss: 0.03222864866256714, Accuracy: 99.39693450927734\n","Epoch 167, Loss: 0.030928583815693855, Accuracy: 99.44392395019531\n","Epoch 168, Loss: 0.030214369297027588, Accuracy: 99.48699951171875\n","Epoch 169, Loss: 0.02952008694410324, Accuracy: 99.49483489990234\n","Epoch 170, Loss: 0.028862042352557182, Accuracy: 99.54182434082031\n","Epoch 171, Loss: 0.028288584202528, Accuracy: 99.53399658203125\n","Epoch 172, Loss: 0.027247866615653038, Accuracy: 99.53791046142578\n","Epoch 173, Loss: 0.026644177734851837, Accuracy: 99.56532287597656\n","Epoch 174, Loss: 0.02555304393172264, Accuracy: 99.59273529052734\n","Epoch 175, Loss: 0.024568133056163788, Accuracy: 99.6005630493164\n","Epoch 176, Loss: 0.024652568623423576, Accuracy: 99.62797546386719\n","Epoch 177, Loss: 0.02386443503201008, Accuracy: 99.67105102539062\n","Epoch 178, Loss: 0.023050513118505478, Accuracy: 99.66322326660156\n","Epoch 179, Loss: 0.02226906642317772, Accuracy: 99.67105102539062\n","Epoch 180, Loss: 0.021811164915561676, Accuracy: 99.6671371459961\n","Epoch 181, Loss: 0.020597930997610092, Accuracy: 99.71412658691406\n","Epoch 182, Loss: 0.020310331135988235, Accuracy: 99.75328826904297\n","Epoch 183, Loss: 0.01972188800573349, Accuracy: 99.7650375366211\n","Epoch 184, Loss: 0.019290026277303696, Accuracy: 99.75328826904297\n","Epoch 185, Loss: 0.018520019948482513, Accuracy: 99.76895141601562\n","Epoch 186, Loss: 0.01825716905295849, Accuracy: 99.77678680419922\n","Epoch 187, Loss: 0.017683546990156174, Accuracy: 99.78853607177734\n","Epoch 188, Loss: 0.017583727836608887, Accuracy: 99.78462219238281\n","Epoch 189, Loss: 0.016892967745661736, Accuracy: 99.80028533935547\n","Epoch 190, Loss: 0.016018928959965706, Accuracy: 99.79244995117188\n","Epoch 191, Loss: 0.015686901286244392, Accuracy: 99.81594848632812\n","Epoch 192, Loss: 0.015282688662409782, Accuracy: 99.81986236572266\n","Epoch 193, Loss: 0.014853025786578655, Accuracy: 99.81594848632812\n","Epoch 194, Loss: 0.014362254180014133, Accuracy: 99.83552551269531\n","Epoch 195, Loss: 0.0139994565397501, Accuracy: 99.83552551269531\n","Epoch 196, Loss: 0.01349565014243126, Accuracy: 99.83552551269531\n","Epoch 197, Loss: 0.013259312137961388, Accuracy: 99.85118865966797\n","Epoch 198, Loss: 0.013128058984875679, Accuracy: 99.84727478027344\n","Epoch 199, Loss: 0.012699272483587265, Accuracy: 99.8433609008789\n","Epoch 200, Loss: 0.012374995276331902, Accuracy: 99.85511016845703\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yBffjaCriYK3"},"source":["## 테스트 루프"]},{"cell_type":"code","metadata":{"id":"eg5cacKRiYK4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630067548755,"user_tz":-540,"elapsed":3778,"user":{"displayName":"이주민","photoUrl":"","userId":"01386847358094111822"}},"outputId":"65cb6fc7-ae52-4687-d1b3-292be4e28a30"},"source":["for test_seq, test_labels in test_ds:\n","    prediction = test_step(model, test_seq)\n","    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n","    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n","    texts = tokenizer.sequences_to_texts(prediction.numpy())\n","    print('_')\n","    print('q: ', test_text)\n","    print('a: ', gt_text)\n","    print('p: ', texts)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["_\n","q:  ['여기 기프티콘 되죠 \\n']\n","a:  ['\\t 네 현금영수증 해드릴까 요 \\n']\n","p:  ['할인 카드 도 안 하시나요 \\n']\n","_\n","q:  ['네 에 테이크 아웃 도 가능한가요 \\n']\n","a:  ['\\t 네 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n","p:  ['네 다음 에 써도 됩니다 \\n']\n","_\n","q:  ['아메리카노 톨 사이즈 로 주세요 \\n']\n","a:  ['\\t 따뜻한 거 로 드릴 까요 \\n']\n","p:  ['드시고 가시나요 \\n']\n","_\n","q:  ['진동 을 따로 주시나요 \\n']\n","a:  ['\\t 주 번호 로 드리겠습니다 \\n']\n","p:  ['네 드릴게요 잠시 만 요 \\n']\n","_\n","q:  ['자리 있나요 \\n']\n","a:  ['\\t 네 있습니다 \\n']\n","p:  ['네 자몽 차는 있습니다 \\n']\n","_\n","q:  ['그럼 루이보스 밀크 티 하나 \\n']\n","a:  ['\\t 네 알겠습니다 \\n']\n","p:  ['여기 사이즈 는 지금 없어요 \\n']\n","_\n","q:  ['다음 에 무료 로 하고 엔 도장 찍어주세요 \\n']\n","a:  ['\\t 네 \\n']\n","p:  ['네 이리 주세요 \\n']\n","_\n","q:  ['아메리카노 한 잔 에 얼마 죠 \\n']\n","a:  ['\\t 입니다 \\n']\n","p:  ['4000원 입니다 \\n']\n","_\n","q:  ['얼마나 \\n']\n","a:  ['\\t 바로 만들어 드릴게요 \\n']\n","p:  ['네 진동 벨 울리면 픽업 테이블 로 와주세요 \\n']\n","_\n","q:  ['카푸치노 는 로 주시 고 아메리카노 는 로 \\n']\n","a:  ['\\t 네 더 없으세요 \\n']\n","p:  ['할인 적용 해서 9300원 결제 도 와 드릴게요 \\n']\n","_\n","q:  ['아메리카노 는 어떤 종류 가 있나요 \\n']\n","a:  ['\\t 디카 페인 과 기본 아메리카노 2 종류 있습니다 \\n']\n","p:  ['헤이즐넛 시럽 은 있습니다 \\n']\n","_\n","q:  ['카카오 페이 로 결제 가능한가요 \\n']\n","a:  ['\\t 네 가능합니다 \\n']\n","p:  ['네 \\n']\n","_\n","q:  ['오늘 의 커피 는 커피 로 하나요 맛 이 \\n']\n","a:  ['\\t 아 네 오늘 은 과테말라 커피 입니다 \\n']\n","p:  ['네 원하시는 원두 와 내리는 방법 을 선택 하실 수 있습니다 \\n']\n","_\n","q:  ['머핀 은 뭐 가 제일 \\n']\n","a:  ['\\t 블루베리 머핀 이 잘 나갑니다 \\n']\n","p:  ['네 1500원 결제 도 와 드리겠습니다 \\n']\n","_\n","q:  ['현금 영수증 해주세요 \\n']\n","a:  ['\\t 네 번호 찍어주세요 \\n']\n","p:  ['네 번호 찍어주세요 \\n']\n","_\n","q:  ['둘 다 톨 사이즈 로 주세요 \\n']\n","a:  ['\\t 여기 서 드시고 요 \\n']\n","p:  ['다른 건 필요 없으신 가요 \\n']\n","_\n","q:  ['아이스 아메리카노 한 잔 가능한가요 \\n']\n","a:  ['\\t 네 가능합니다 \\n']\n","p:  ['네 바닥 청소 도 해야 해서 죄송합니다 \\n']\n","_\n","q:  ['아이스 아메리카노 에 샷 이 몇 개 \\n']\n","a:  ['\\t 아이스 아메리카노 에 샷 은 개 \\n']\n","p:  ['네 그럼 카페라떼 로 드릴 까요 \\n']\n","_\n","q:  ['카페라테 한 잔 주세요 \\n']\n","a:  ['\\t 카페라테 따뜻한 걸 로 드릴 까요 \\n']\n","p:  ['네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n","_\n","q:  ['아니요 \\n']\n","a:  ['\\t 네 더 필요하신 건 없으신 가요 \\n']\n","p:  ['네 원하시는 원두 와 내리는 방법 을 선택 하실 수 있습니다 \\n']\n","_\n","q:  ['네 찍어주세요 \\n']\n","a:  ['\\t 네 주문 딸기 스무디 와 쿠키 드릴게요 \\n']\n","p:  ['네 더 필요하신 건 없으세요 \\n']\n","_\n","q:  ['시즌 메뉴 오늘 도 가능한가요 \\n']\n","a:  ['\\t 네 시즌 메뉴 가능합니다 \\n']\n","p:  ['네 카페라테 디카 페인 500원 추가 됩니다 \\n']\n","_\n","q:  ['시즌 메뉴 와 함께 되어 있는 세트 메뉴 가 있나요 \\n']\n","a:  ['\\t 네 치즈 케이크 와 시즌 메뉴 두 잔 으로 세트 메뉴 있습니다 \\n']\n","p:  ['네 캐리어 에 담아 드릴게요 \\n']\n","_\n","q:  ['라테 에 우유 두 도 변경 가능한가요 \\n']\n","a:  ['\\t 네 라테 에 두유 로 변경 가능합니다 \\n']\n","p:  ['네 카페라테 디카 페인 라테 추가 케이크 다 아이스 가능합니다 \\n']\n","_\n","q:  ['네 먹고 갈 거 예요 \\n']\n","a:  ['\\t 카드 여기 주세요 \\n']\n","p:  ['포크 는 몇 개 드릴 까요 \\n']\n","_\n","q:  ['카페인 이 음료 있나요 \\n']\n","a:  ['\\t 티 음료 와 스무디 에는 카페인 이 않습니다 \\n']\n","p:  ['네 가능합니다 \\n']\n","_\n","q:  ['딸기스무디 랑 키위 스무디 는 생 과일 인가요 \\n']\n","a:  ['\\t 딸기 는 키위 는 생 과일 을 사용 하고 있습니다 \\n']\n","p:  ['500 원 추가 입니다 \\n']\n","_\n","q:  ['그럼 딸기 스무디 하나 주세요 \\n']\n","a:  ['\\t 드시고 가시나요 \\n']\n","p:  ['사이즈 는 어떻게 드릴 까요 \\n']\n","_\n","q:  ['아메리카노 한 잔이요 \\n']\n","a:  ['\\t 아이스 아메리카노 로 드릴 까요 \\n']\n","p:  ['드시고 가시나요 \\n']\n","_\n","q:  ['네 도 같이 \\n']\n","a:  ['\\t 네 아메리카노 4000원 입니다 \\n']\n","p:  ['네 번호 입력 해주세요 \\n']\n","_\n","q:  ['디카 페인 아이스 아메리카노 한 잔 주세요 \\n']\n","a:  ['\\t 디카 페인 아이스 아메리카노 는 기존 금액 에 300원 추가 되는데 괜찮으신 가요 \\n']\n","p:  ['아이스 아메리카노 포장 이신 가요 \\n']\n","_\n","q:  ['커피 음료 것 뭐 가 있나요 \\n']\n","a:  ['\\t 스무디 와 주스 있습니다 \\n']\n","p:  ['네 다음 에 써도 됩니다 \\n']\n","_\n","q:  ['주스 어떤 종류 있나요 \\n']\n","a:  ['\\t 딸기 주스 주스 주스 가 있습니다 \\n']\n","p:  ['네 여기 있습니다 \\n']\n","_\n","q:  ['플랫 화이트 라지 로 주세요 \\n']\n","a:  ['\\t 네 \\n']\n","p:  ['드시고 가시나요 \\n']\n","_\n","q:  ['네 레드 벨벳 케이크 주세요 \\n']\n","a:  ['\\t 음료 는 뭘 로 드릴 까요 \\n']\n","p:  ['드시고 가시나요 \\n']\n","_\n","q:  ['네 먹고 갈 거 예요 \\n']\n","a:  ['\\t 유리잔 괜찮으세요 \\n']\n","p:  ['포크 는 몇 개 드릴 까요 \\n']\n","_\n","q:  ['따뜻한 밀크 티 주세요 \\n']\n","a:  ['\\t 네 \\n']\n","p:  ['사이즈 는 어떻게 드릴 까요 \\n']\n","_\n","q:  ['음료 얼마나 하나요 \\n']\n","a:  ['\\t 10분 정도 주시 면 됩니다 \\n']\n","p:  ['매장 에서 드실 거 면 머그컵 에 드리도록 되어있습니다 \\n']\n","_\n","q:  ['아이스 아메리카노 한잔 얼마 인가요 \\n']\n","a:  ['\\t 4500원 입니다 \\n']\n","p:  ['아이스 아메리카노 1 잔 은 4000원 아이스 카페라테 1 잔 은 5000원 입니다 \\n']\n","_\n","q:  ['현금영수증 번호 \\n']\n","a:  ['\\t 네 \\n']\n","p:  ['따뜻한 거 맞으세요 \\n']\n","_\n","q:  ['이 카드 로 결제 해주세요 \\n']\n","a:  ['\\t 네 결제 도 와 드릴게요 \\n']\n","p:  ['네 알겠습니다 \\n']\n","_\n","q:  ['주문 한 게 다 안 \\n']\n","a:  ['\\t 주 번호 가 몇 이 죠 \\n']\n","p:  ['네 더 필요한 건 없으세요 \\n']\n","_\n","q:  ['을 \\n']\n","a:  ['\\t \\n']\n","p:  ['쓴맛 은 많이 없고 산미 가 있고 다른 원두 들 보다 과일 향 이 나죠 \\n']\n","_\n","q:  ['베이글 은 얼마 인가요 \\n']\n","a:  ['\\t 베이글 은 2000원 입니다 \\n']\n","p:  ['5000원 입니다 \\n']\n","_\n","q:  ['지금 되나요 \\n']\n","a:  ['\\t 는 계절 메뉴 라 지금 은 판매 하지 않습니다 \\n']\n","p:  ['건물 뒤 에 주차장 있습니다 \\n']\n","_\n","q:  ['바닐라 라테 는 따뜻하게 주세요 \\n']\n","a:  ['\\t 네 적립 이나 할인 카드 있으세요 \\n']\n","p:  ['사이즈 는 어떻게 드릴 까요 \\n']\n","_\n","q:  ['테이크 아웃 으로 부탁드립니다 \\n']\n","a:  ['\\t 결제 는 이 쪽 에서 도 와 드릴게요 \\n']\n","p:  ['10분 정도 걸려요 \\n']\n","_\n","q:  ['혹시 테이크 아웃 잔 에 수 있나요 \\n']\n","a:  ['\\t 테이크 아웃 하시는 건가 요 \\n']\n","p:  ['네 캐리어 에 담아 드릴게요 \\n']\n","_\n","q:  ['아메리카노 하나 는 샷 추가 해주세요 \\n']\n","a:  ['\\t 아메리카노 는 둘 다 따뜻한 걸 로 드릴 까요 \\n']\n","p:  ['결제 도 와 드릴게요 \\n']\n","_\n","q:  ['쿠폰 찍어주세요 \\n']\n","a:  ['\\t 네 찍어 드릴게요 \\n']\n","p:  ['10 개 다모아 오시 면 커피한잔 드려요 \\n']\n","_\n","q:  ['주문 할게요 \\n']\n","a:  ['\\t 어떤 거 드릴 까요 \\n']\n","p:  ['플랫 화이트 사이즈 는 뭘 로 하시겠어요 \\n']\n","_\n","q:  ['파나요 \\n']\n","a:  ['\\t 는 계절 지금 은 \\n']\n","p:  ['네 4500원 입니다 \\n']\n","_\n","q:  ['그럼 겨울 메뉴 뭐 가 \\n']\n","a:  ['\\t 겨울 엔 감귤 라테 가 제일 많이 나가요 \\n']\n","p:  ['네 원하시는 원두 와 내리는 방법 을 선택 하실 수 있습니다 \\n']\n","_\n","q:  ['네 결제 는 카드 로 할게요 \\n']\n","a:  ['\\t 네 결제 완료 되었습니다 \\n']\n","p:  ['네 잠시 만 기다려주세요 \\n']\n","_\n","q:  ['둘 다 사이즈 로 할게요 \\n']\n","a:  ['\\t 네 결제 는 어떤 것 으로 도 와 드릴 까요 \\n']\n","p:  ['사이즈 선택 해주세요 \\n']\n","_\n","q:  ['기프티콘 으로 결제 할게요 \\n']\n","a:  ['\\t 네 그럼 쿠폰 저 \\n']\n","p:  ['저 한테 보여주시고 제 가 확인 버튼 누르면 돼요 \\n']\n","_\n","q:  ['녹차 라테 1 잔 주세요 \\n']\n","a:  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n","p:  ['네 드시고 가시나요 \\n']\n","_\n","q:  ['네 그럼 휘핑크림 추가 해서 주세요 \\n']\n","a:  ['\\t 네 녹차 라테 에 휘핑크림 추가 해서 4500원 입니다 \\n']\n","p:  ['네 사이즈 는 어떻게 드릴 까요 \\n']\n","_\n","q:  ['브레드 종류 는 뭐 가 있나요 \\n']\n","a:  ['\\t 허니 브레드 와 갈릭 치즈 브레드 가 있습니다 \\n']\n","p:  ['들고 가실 건가 요 \\n']\n","_\n","q:  ['생크림 이 건 어떤 건가 요 \\n']\n","a:  ['\\t 허니 브레드 입니다 \\n']\n","p:  ['앉아계실 거 면 컵 만 가능하세요 \\n']\n","_\n","q:  ['네 그렇게 만들어 주세요 \\n']\n","a:  ['\\t 더 필요한 건 없으세요 \\n']\n","p:  ['네 더 필요한 건 없으세요 \\n']\n","_\n","q:  ['여기 있습니다 \\n']\n","a:  ['\\t 네 확인 되셨고 되면 진동 벨 거 예요 \\n']\n","p:  ['가실 때 말씀 해주시면 테이크 아웃 잔 에 어떤 걸 드릴 드릴 까요 \\n']\n","_\n","q:  ['핫초코 한 잔 아메리카노 사이 즈 업 한 잔 하면 얼마 인가요 \\n']\n","a:  ['\\t 입니다 \\n']\n","p:  ['9500원 입니다 \\n']\n","_\n","q:  ['주스 는 다른 건 없나요 \\n']\n","a:  ['\\t 그럼 에 라테 추천 \\n']\n","p:  ['치즈 케이크 가 잘 팔려요 \\n']\n","_\n","q:  ['그건 \\n']\n","a:  ['\\t 네 만 따듯 해 요 \\n']\n","p:  ['과일 생크림 케이크 가 잘 나가요 \\n']\n","_\n","q:  ['통신사 할인 되죠 \\n']\n","a:  ['\\t 네 300원 할인 됩니다 \\n']\n","p:  ['아메리카노 원두 는 어떤 걸 로 할까 요 \\n']\n","_\n","q:  ['매장 에서 언제 까지 영업 하시나요 \\n']\n","a:  ['\\t 오후 10시 까지 영업 입니다 \\n']\n","p:  ['네 있습니다 \\n']\n","_\n","q:  ['아니요 그냥 주세요 \\n']\n","a:  ['\\t 결제 해드릴게요 \\n']\n","p:  ['카페모카 5천 원 입니다 \\n']\n","_\n","q:  ['가격 안 되나요 \\n']\n","a:  ['\\t 한 해드릴게요 \\n']\n","p:  ['네 배달 비 3000원 입니다 \\n']\n","_\n","q:  ['카페라테 한잔 주세요 \\n']\n","a:  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n","p:  ['드시고 가시나요 \\n']\n","_\n","q:  ['네 차가운 걸 로 주세요 \\n']\n","a:  ['\\t 4500원 입니다 \\n']\n","p:  ['드시고 가시나요 \\n']\n","_\n","q:  ['어떤 게 괜찮아요 \\n']\n","a:  ['\\t 이나 원두 를 하시는 게 아니면 예 가체 많이 추천 \\n']\n","p:  ['어니언 크림 이랑 플레인 크림 있습니다 \\n']\n","_\n","q:  ['그럼 추천 치즈 케이크 도 같이 주세요 \\n']\n","a:  ['\\t 네 매장 에서 드시고 가시나요 \\n']\n","p:  ['네 더 필요한 거 없으신 가요 \\n']\n","_\n","q:  ['그리고 휘핑크림 은 에스프레소 크림 으로 \\n']\n","a:  ['\\t 결제 는 어떻게 해드릴까 요 \\n']\n","p:  ['네 더 필요한 건 없으세요 \\n']\n","_\n","q:  ['지금 도 할인 하나요 \\n']\n","a:  ['\\t 네 10시 까지 하고 있습니다 \\n']\n","p:  ['네 일회용 컵 에 담아 드리겠습니다 \\n']\n","_\n","q:  ['그럼 와 아이스 아메리카노 로 할게요 \\n']\n","a:  ['\\t 더 필요하신 건 없나요 \\n']\n","p:  ['드시고 가시나요 \\n']\n","_\n","q:  ['네 할인 적립 은 \\n']\n","a:  ['\\t 네 바코드 \\n']\n","p:  ['네 알겠습니다 \\n']\n","_\n","q:  ['초코 프라푸치노 주세요 \\n']\n","a:  ['\\t 휘핑 올려 드릴 까요 \\n']\n","p:  ['네 아이스 프라푸치노 몇 잔 드릴 까요 \\n']\n","_\n","q:  ['시럽 도 가 \\n']\n","a:  ['\\t 드시고 가시나요 \\n']\n","p:  ['네 원하시는 원두 와 내리는 방법 을 선택 하실 수 있습니다 \\n']\n","_\n","q:  ['둘 다 사이즈 로 주세요 \\n']\n","a:  ['\\t 드시고 가시나요 \\n']\n","p:  ['카페모카 위 에 휘핑 올려 드릴 까요 \\n']\n","_\n","q:  ['마시다가 갈 건데 테이크아웃 으로 주세요 \\n']\n","a:  ['\\t 상 매장 에서는 머그컵 으로 드리고 있어요 \\n']\n","p:  ['네 진동 벨 울리면 찾으러 오세요 \\n']\n","_\n","q:  ['나갈 때 테이크아웃 컵 으로 수 있나요 \\n']\n","a:  ['\\t 네 그렇게 해드릴게요 \\n']\n","p:  ['아뇨 캐리어 는 2 개 이상 부터 가능해요 \\n']\n","_\n","q:  ['아메리카노 두 잔 한잔 주세요 \\n']\n","a:  ['\\t 드시고 가실 건가 요 \\n']\n","p:  ['휘핑크림 올려 드릴 까요 \\n']\n","_\n","q:  ['얼마나 하나요 \\n']\n","a:  ['\\t 5분 정도 \\n']\n","p:  ['매장 에서 드시고 가세 요 \\n']\n","_\n","q:  ['포인트 적립 해주세요 \\n']\n","a:  ['\\t 네 번호 입력 부탁드립니다 \\n']\n","p:  ['네 카드 적립 되었습니다 \\n']\n","_\n","q:  ['네 번호 로 할게요 \\n']\n","a:  ['\\t 네 에 번호 \\n']\n","p:  ['네 더 필요한 건 없으세요 \\n']\n","_\n","q:  ['아 포인트 포인트 사용 해주세요 \\n']\n","a:  ['\\t 네 고객 님 포인트 총 있으신 데 사용 도 와 드리겠습니다 \\n']\n","p:  ['네 가능하세요 \\n']\n","_\n","q:  ['톨 사이즈 로 주문 할게요 \\n']\n","a:  ['\\t 네 계산 도 와 드리겠습니다 \\n']\n","p:  ['아메리카노 원두 는 어떤 걸 로 할까 요 \\n']\n","_\n","q:  ['아메리카노 사이즈 가능한가요 \\n']\n","a:  ['\\t 네 500원 만 추가 하시면 가능하십니다 \\n']\n","p:  ['네 당연히 가능합니다 \\n']\n","_\n","q:  ['사이 즈 업 해서 주세요 \\n']\n","a:  ['\\t 네 결제 는 어떻게 도 와 드릴 까요 \\n']\n","p:  ['네 더 필요한 거 없으신 가요 \\n']\n","_\n","q:  ['커피 는 텀블러 에 담아주세요 \\n']\n","a:  ['\\t 네 텀블러 할인 4000원 결제 도 와 드리겠습니다 \\n']\n","p:  ['텀블러 할인 300원 같이 해드릴게요 \\n']\n","_\n","q:  ['아니요 아이스 로 주세요 \\n']\n","a:  ['\\t 드시고 가실 건가 요 \\n']\n","p:  ['레귤러 사이즈 로 괜찮으세요 \\n']\n","_\n","q:  ['테이크아웃 할게요 \\n']\n","a:  ['\\t 지금 중 인데 케이크 주문 하시면 아메리카노 한잔 로 드려요 \\n']\n","p:  ['결제 완료 되었습니다 \\n']\n","_\n","q:  ['현금 결제 가 안 \\n']\n","a:  ['\\t 현금 은 에서 주문 도 와 드리겠습니다 \\n']\n","p:  ['네 카드 랑 쿠폰 까요 \\n']\n","_\n","q:  ['포인트 적립 되나요 \\n']\n","a:  ['\\t 번호 포인트 적립 도 와 드리고 있어요 \\n']\n","p:  ['네 됩니다 \\n']\n","_\n","q:  ['포인트 적립 할게요 \\n']\n","a:  ['\\t 네 결제 되셨습니다 \\n']\n","p:  ['네 카드 적립 되었습니다 \\n']\n","_\n","q:  ['티라미수 는 있나요 \\n']\n","a:  ['\\t 네 티라미수 는 있습니다 \\n']\n","p:  ['네 500원 추가 하시면 추가 돼요 \\n']\n","_\n","q:  ['네 현금영수증 해주세요 \\n']\n","a:  ['\\t 네 드시고 가시나요 \\n']\n","p:  ['네 번호 입력 해주세요 \\n']\n","_\n","q:  ['샷 추가 해주세요 \\n']\n","a:  ['\\t 네 알겠습니다 \\n']\n","p:  ['알겠습니다 \\n']\n","_\n","q:  ['얼마 에요 \\n']\n","a:  ['\\t 만 원 입니다 \\n']\n","p:  ['총 7000원 입니다 \\n']\n","_\n","q:  ['아이스 아메리카노 랑 샌드위치 주세요 \\n']\n","a:  ['\\t 10시 에 세트 할인 가능하세요 \\n']\n","p:  ['카페모카 5천 원 입니다 \\n']\n"],"name":"stdout"}]}]}